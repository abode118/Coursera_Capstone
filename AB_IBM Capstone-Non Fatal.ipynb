{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anne's IBM Data Science Coursera Capstone:\n",
    "### Predicting Accident Severity in Great Britain\n",
    "\n",
    "In this notebook we create a classification model to predict whether a car accident in Great Britain would be fatal or non-fatal, given a set of features/conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Capstone Project Course!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import resample\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "print('Hello Capstone Project Course!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Accident Data From CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abode\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident_Index</th>\n",
       "      <th>Location_Easting_OSGR</th>\n",
       "      <th>Location_Northing_OSGR</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Police_Force</th>\n",
       "      <th>Accident_Severity</th>\n",
       "      <th>Number_of_Vehicles</th>\n",
       "      <th>Number_of_Casualties</th>\n",
       "      <th>Date</th>\n",
       "      <th>...</th>\n",
       "      <th>Pedestrian_Crossing.Human_Control</th>\n",
       "      <th>Pedestrian_Crossing.Physical_Facilities</th>\n",
       "      <th>Light_Conditions</th>\n",
       "      <th>Weather_Conditions</th>\n",
       "      <th>Road_Surface_Conditions</th>\n",
       "      <th>Special_Conditions_at_Site</th>\n",
       "      <th>Carriageway_Hazards</th>\n",
       "      <th>Urban_or_Rural_Area</th>\n",
       "      <th>Did_Police_Officer_Attend_Scene_of_Accident</th>\n",
       "      <th>LSOA_of_Accident_Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200501BS00001</td>\n",
       "      <td>525680.0</td>\n",
       "      <td>178240.0</td>\n",
       "      <td>-0.191170</td>\n",
       "      <td>51.489096</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>04/01/2005</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>E01002849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200501BS00002</td>\n",
       "      <td>524170.0</td>\n",
       "      <td>181650.0</td>\n",
       "      <td>-0.211708</td>\n",
       "      <td>51.520075</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>05/01/2005</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>E01002909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200501BS00003</td>\n",
       "      <td>524520.0</td>\n",
       "      <td>182240.0</td>\n",
       "      <td>-0.206458</td>\n",
       "      <td>51.525301</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>06/01/2005</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>E01002857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200501BS00004</td>\n",
       "      <td>526900.0</td>\n",
       "      <td>177530.0</td>\n",
       "      <td>-0.173862</td>\n",
       "      <td>51.482442</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>07/01/2005</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>E01002840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200501BS00005</td>\n",
       "      <td>528060.0</td>\n",
       "      <td>179040.0</td>\n",
       "      <td>-0.156618</td>\n",
       "      <td>51.495752</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10/01/2005</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>E01002863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Accident_Index  Location_Easting_OSGR  Location_Northing_OSGR  Longitude  \\\n",
       "0  200501BS00001               525680.0                178240.0  -0.191170   \n",
       "1  200501BS00002               524170.0                181650.0  -0.211708   \n",
       "2  200501BS00003               524520.0                182240.0  -0.206458   \n",
       "3  200501BS00004               526900.0                177530.0  -0.173862   \n",
       "4  200501BS00005               528060.0                179040.0  -0.156618   \n",
       "\n",
       "    Latitude  Police_Force  Accident_Severity  Number_of_Vehicles  \\\n",
       "0  51.489096             1                  2                   1   \n",
       "1  51.520075             1                  3                   1   \n",
       "2  51.525301             1                  3                   2   \n",
       "3  51.482442             1                  3                   1   \n",
       "4  51.495752             1                  3                   1   \n",
       "\n",
       "   Number_of_Casualties        Date            ...              \\\n",
       "0                     1  04/01/2005            ...               \n",
       "1                     1  05/01/2005            ...               \n",
       "2                     1  06/01/2005            ...               \n",
       "3                     1  07/01/2005            ...               \n",
       "4                     1  10/01/2005            ...               \n",
       "\n",
       "   Pedestrian_Crossing.Human_Control Pedestrian_Crossing.Physical_Facilities  \\\n",
       "0                                  0                                       1   \n",
       "1                                  0                                       5   \n",
       "2                                  0                                       0   \n",
       "3                                  0                                       0   \n",
       "4                                  0                                       0   \n",
       "\n",
       "   Light_Conditions Weather_Conditions  Road_Surface_Conditions  \\\n",
       "0                 1                  2                        2   \n",
       "1                 4                  1                        1   \n",
       "2                 4                  1                        1   \n",
       "3                 1                  1                        1   \n",
       "4                 7                  1                        2   \n",
       "\n",
       "   Special_Conditions_at_Site  Carriageway_Hazards  Urban_or_Rural_Area  \\\n",
       "0                           0                    0                    1   \n",
       "1                           0                    0                    1   \n",
       "2                           0                    0                    1   \n",
       "3                           0                    0                    1   \n",
       "4                           0                    0                    1   \n",
       "\n",
       "   Did_Police_Officer_Attend_Scene_of_Accident  LSOA_of_Accident_Location  \n",
       "0                                            1                  E01002849  \n",
       "1                                            1                  E01002909  \n",
       "2                                            1                  E01002857  \n",
       "3                                            1                  E01002840  \n",
       "4                                            1                  E01002863  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import dataset and view\n",
    "df = pd.read_csv('acc2005_2016.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location_Easting_OSGR</th>\n",
       "      <th>Location_Northing_OSGR</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Police_Force</th>\n",
       "      <th>Accident_Severity</th>\n",
       "      <th>Number_of_Vehicles</th>\n",
       "      <th>Number_of_Casualties</th>\n",
       "      <th>Day_of_Week</th>\n",
       "      <th>Local_Authority_.District.</th>\n",
       "      <th>...</th>\n",
       "      <th>X2nd_Road_Number</th>\n",
       "      <th>Pedestrian_Crossing.Human_Control</th>\n",
       "      <th>Pedestrian_Crossing.Physical_Facilities</th>\n",
       "      <th>Light_Conditions</th>\n",
       "      <th>Weather_Conditions</th>\n",
       "      <th>Road_Surface_Conditions</th>\n",
       "      <th>Special_Conditions_at_Site</th>\n",
       "      <th>Carriageway_Hazards</th>\n",
       "      <th>Urban_or_Rural_Area</th>\n",
       "      <th>Did_Police_Officer_Attend_Scene_of_Accident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1917129.00</td>\n",
       "      <td>1917129.00</td>\n",
       "      <td>1917129.00</td>\n",
       "      <td>1917129.00</td>\n",
       "      <td>1917274.00</td>\n",
       "      <td>1917274.00</td>\n",
       "      <td>1917274.00</td>\n",
       "      <td>1917274.00</td>\n",
       "      <td>1917274.00</td>\n",
       "      <td>1917274.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1917274.00</td>\n",
       "      <td>1917274.00</td>\n",
       "      <td>1917274.00</td>\n",
       "      <td>1917274.00</td>\n",
       "      <td>1917274.00</td>\n",
       "      <td>1917274.00</td>\n",
       "      <td>1917274.00</td>\n",
       "      <td>1917274.00</td>\n",
       "      <td>1917274.00</td>\n",
       "      <td>1917274.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>440787.01</td>\n",
       "      <td>297787.56</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>52.57</td>\n",
       "      <td>30.69</td>\n",
       "      <td>2.84</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.35</td>\n",
       "      <td>4.12</td>\n",
       "      <td>352.52</td>\n",
       "      <td>...</td>\n",
       "      <td>374.24</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>95483.63</td>\n",
       "      <td>160953.29</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.45</td>\n",
       "      <td>25.50</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.92</td>\n",
       "      <td>259.26</td>\n",
       "      <td>...</td>\n",
       "      <td>1290.94</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>64950.00</td>\n",
       "      <td>10290.00</td>\n",
       "      <td>-7.52</td>\n",
       "      <td>49.91</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>377346.00</td>\n",
       "      <td>177890.00</td>\n",
       "      <td>-2.34</td>\n",
       "      <td>51.49</td>\n",
       "      <td>7.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>120.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>442260.00</td>\n",
       "      <td>263260.00</td>\n",
       "      <td>-1.37</td>\n",
       "      <td>52.25</td>\n",
       "      <td>31.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>327.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>523810.00</td>\n",
       "      <td>396070.00</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>53.46</td>\n",
       "      <td>46.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>531.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>655540.00</td>\n",
       "      <td>1208800.00</td>\n",
       "      <td>1.76</td>\n",
       "      <td>60.76</td>\n",
       "      <td>98.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>67.00</td>\n",
       "      <td>93.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>941.00</td>\n",
       "      <td>...</td>\n",
       "      <td>9999.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Location_Easting_OSGR  Location_Northing_OSGR   Longitude    Latitude  \\\n",
       "count             1917129.00              1917129.00  1917129.00  1917129.00   \n",
       "mean               440787.01               297787.56       -1.42       52.57   \n",
       "std                 95483.63               160953.29        1.40        1.45   \n",
       "min                 64950.00                10290.00       -7.52       49.91   \n",
       "25%                377346.00               177890.00       -2.34       51.49   \n",
       "50%                442260.00               263260.00       -1.37       52.25   \n",
       "75%                523810.00               396070.00       -0.21       53.46   \n",
       "max                655540.00              1208800.00        1.76       60.76   \n",
       "\n",
       "       Police_Force  Accident_Severity  Number_of_Vehicles  \\\n",
       "count    1917274.00         1917274.00          1917274.00   \n",
       "mean          30.69               2.84                1.83   \n",
       "std           25.50               0.40                0.71   \n",
       "min            1.00               1.00                1.00   \n",
       "25%            7.00               3.00                1.00   \n",
       "50%           31.00               3.00                2.00   \n",
       "75%           46.00               3.00                2.00   \n",
       "max           98.00               3.00               67.00   \n",
       "\n",
       "       Number_of_Casualties  Day_of_Week  Local_Authority_.District.  \\\n",
       "count            1917274.00   1917274.00                  1917274.00   \n",
       "mean                   1.35         4.12                      352.52   \n",
       "std                    0.82         1.92                      259.26   \n",
       "min                    1.00         1.00                        1.00   \n",
       "25%                    1.00         2.00                      120.00   \n",
       "50%                    1.00         4.00                      327.00   \n",
       "75%                    1.00         6.00                      531.00   \n",
       "max                   93.00         7.00                      941.00   \n",
       "\n",
       "                          ...                       X2nd_Road_Number  \\\n",
       "count                     ...                             1917274.00   \n",
       "mean                      ...                                 374.24   \n",
       "std                       ...                                1290.94   \n",
       "min                       ...                                  -1.00   \n",
       "25%                       ...                                   0.00   \n",
       "50%                       ...                                   0.00   \n",
       "75%                       ...                                   0.00   \n",
       "max                       ...                                9999.00   \n",
       "\n",
       "       Pedestrian_Crossing.Human_Control  \\\n",
       "count                         1917274.00   \n",
       "mean                                0.01   \n",
       "std                                 0.13   \n",
       "min                                -1.00   \n",
       "25%                                 0.00   \n",
       "50%                                 0.00   \n",
       "75%                                 0.00   \n",
       "max                                 2.00   \n",
       "\n",
       "       Pedestrian_Crossing.Physical_Facilities  Light_Conditions  \\\n",
       "count                               1917274.00        1917274.00   \n",
       "mean                                      0.74              1.95   \n",
       "std                                       1.83              1.65   \n",
       "min                                      -1.00             -1.00   \n",
       "25%                                       0.00              1.00   \n",
       "50%                                       0.00              1.00   \n",
       "75%                                       0.00              4.00   \n",
       "max                                       8.00              7.00   \n",
       "\n",
       "       Weather_Conditions  Road_Surface_Conditions  \\\n",
       "count          1917274.00               1917274.00   \n",
       "mean                 1.57                     1.35   \n",
       "std                  1.65                     0.62   \n",
       "min                 -1.00                    -1.00   \n",
       "25%                  1.00                     1.00   \n",
       "50%                  1.00                     1.00   \n",
       "75%                  1.00                     2.00   \n",
       "max                  9.00                     5.00   \n",
       "\n",
       "       Special_Conditions_at_Site  Carriageway_Hazards  Urban_or_Rural_Area  \\\n",
       "count                  1917274.00           1917274.00           1917274.00   \n",
       "mean                         0.11                 0.07                 1.36   \n",
       "std                          0.73                 0.62                 0.48   \n",
       "min                         -1.00                -1.00                 1.00   \n",
       "25%                          0.00                 0.00                 1.00   \n",
       "50%                          0.00                 0.00                 1.00   \n",
       "75%                          0.00                 0.00                 2.00   \n",
       "max                          7.00                 7.00                 3.00   \n",
       "\n",
       "       Did_Police_Officer_Attend_Scene_of_Accident  \n",
       "count                                   1917274.00  \n",
       "mean                                          1.20  \n",
       "std                                           0.41  \n",
       "min                                          -1.00  \n",
       "25%                                           1.00  \n",
       "50%                                           1.00  \n",
       "75%                                           1.00  \n",
       "max                                           3.00  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#descriptive statistics of dataset\n",
    "round(df.describe(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-e7630fa090e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "\n",
    "Here we want to exclude features (columns) that are not predictive. These include features that are location based, or related to post-accident factors like whether police arrived on the scene\n",
    "\n",
    "Also want to remove null values, if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude nonpredictive features\n",
    "df_cut = df[['Accident_Severity',\n",
    "             'Day_of_Week',\n",
    "             'Light_Conditions',\n",
    "             'Weather_Conditions',\n",
    "             'Road_Surface_Conditions',\n",
    "             'Special_Conditions_at_Site',\n",
    "             'Carriageway_Hazards',\n",
    "             'Urban_or_Rural_Area',\n",
    "             'Road_Type',\n",
    "             'Speed_limit']]\n",
    "df_cut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data types\n",
    "df_cut.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if any null values\n",
    "df_cut.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cut.dropna(inplace=True)\n",
    "df_cut.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction & Data Balancing\n",
    "\n",
    "First use correlation analysis to determine which features may be correlated to Accident Severity\n",
    "\n",
    "Then convert the data labels to their word form (use dictionary) to prepare for data visualization (so labels make sense to the user)\n",
    "\n",
    "Finally, balance the data since the vast majority of accidents are \"Slight\", which would bias our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = df_cut.corr()\n",
    "corr_df.sort_values(by = ['Accident_Severity'],inplace=True, ascending=False)\n",
    "corr_df['Accident_Severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_AS = {'1':'Fatal',\n",
    "           '2':'Non-Fatal',\n",
    "           '3':'Non-Fatal'}\n",
    "\n",
    "dict_WC = {'1':'Fine no high winds',\n",
    "           '2':'Raining no high winds',\n",
    "           '3':'Snowing no high winds',\n",
    "           '4':'Fine + high winds',\n",
    "           '5':'Raining + high winds',\n",
    "           '6':'Snowing + high winds',\n",
    "           '7':'Fog or mist',\n",
    "           '8':'DROP',\n",
    "           '9':'DROP',\n",
    "           '-1':'DROP'}\n",
    "\n",
    "dict_LC = {'1':'Daylight',\n",
    "          '4':'Darkness - lights lit',\n",
    "          '5':'Darkness - lights unlit',\n",
    "          '6':'Darkness - no lighting',\n",
    "          '7':'DROP',\n",
    "          '-1':'DROP'}\n",
    "\n",
    "dict_UR = {'1':'Urban',\n",
    "          '2':'Rural',\n",
    "          '3':'DROP'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dict_AS.keys():\n",
    "    df_cut['Accident_Severity'].replace(to_replace=[int(i)],value=[dict_AS[i]],inplace=True)\n",
    "\n",
    "for j in dict_WC.keys():\n",
    "    df_cut['Weather_Conditions'].replace(to_replace=[int(j)],value=[dict_WC[j]],inplace=True)\n",
    "\n",
    "for k in dict_LC.keys():\n",
    "    df_cut['Light_Conditions'].replace(to_replace=[int(k)],value=[dict_LC[k]],inplace=True)\n",
    "\n",
    "for l in dict_UR.keys():\n",
    "    df_cut['Urban_or_Rural_Area'].replace(to_replace=[int(l)],value=[dict_UR[l]],inplace=True)\n",
    "\n",
    "df_cut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = df_cut[df_cut['Weather_Conditions'] == 'DROP'].index\n",
    "df_cut.drop(indexNames, inplace=True)\n",
    "\n",
    "indexNames = df_cut[df_cut['Light_Conditions'] == 'DROP'].index\n",
    "df_cut.drop(indexNames, inplace=True)\n",
    "\n",
    "indexNames = df_cut[df_cut['Urban_or_Rural_Area'] == 'DROP'].index\n",
    "df_cut.drop(indexNames, inplace=True)\n",
    "\n",
    "df_cut['Weather_Conditions'].value_counts()\n",
    "\n",
    "df_cut['Speed_limit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check whether dataset is balanced/unbalanced re: Accident_Severity label\n",
    "df_cut['Accident_Severity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balance the dataset by undersampling / oversampling\n",
    "Severity_1 = df_cut[df_cut['Accident_Severity'] == 'Fatal']\n",
    "Severity_2 = df_cut[df_cut['Accident_Severity'] == 'Non-Fatal']\n",
    "\n",
    "Severity_1_upsampled = resample(Severity_1, \n",
    "                                replace=True,     # sample with replacement\n",
    "                                n_samples=100000,    # to match majority class\n",
    "                                random_state=1) # reproducible results\n",
    " \n",
    "Severity_2_undersampled = resample(Severity_2,\n",
    "                                   replace=False,    # sample without replacement\n",
    "                                   n_samples=100000,     # to match minority class\n",
    "                                   random_state=1)\n",
    "\n",
    "df_balanced = pd.concat([Severity_1_upsampled, Severity_2_undersampled])\n",
    "df_balanced.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Display new class counts\n",
    "df_balanced['Accident_Severity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Visualization\n",
    "\n",
    "Here we visualize the relationship between the most highly correlated features and our target variable (Accident_Severity)\n",
    "\n",
    "The four features most strongly correlated (negatively and positively) with the target variable (Accident_Severity) are visualized in two ways. 1: stacked histogram 2: stacked percentage of accident severity for each label\n",
    "\n",
    "These features: Weather Conditions, Urban or Rural Area, Light Conditions, and Speed Limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Weather Conditions and Accident Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacked histogram\n",
    "df1 = df_balanced.groupby(['Weather_Conditions', 'Accident_Severity'])['Weather_Conditions'].count().unstack('Accident_Severity')\n",
    "df1[['Fatal','Non-Fatal']].plot(kind='bar',\n",
    "                  stacked=True,\n",
    "                  figsize=(8,6),\n",
    "                  colormap='winter',\n",
    "                  title='Weather Conditions and Accident Severity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacked percentage\n",
    "#round(df_balanced.groupby(['Weather_Conditions'])['Accident_Severity'].value_counts(normalize=True)*100,2)\n",
    "df1_pct = df_balanced.groupby(['Weather_Conditions'])['Accident_Severity'].value_counts(normalize=True).unstack('Accident_Severity')\n",
    "df1_pct[['Fatal','Non-Fatal']].plot(kind='bar',\n",
    "                      stacked=True,\n",
    "                      figsize=(8,6),\n",
    "                      colormap='winter',\n",
    "                      title='Weather Conditions and Accident Severity, Stacked %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Urban/Rural and Accident Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacked histogram\n",
    "df2 = df_balanced.groupby(['Urban_or_Rural_Area', 'Accident_Severity'])['Urban_or_Rural_Area'].count().unstack('Accident_Severity')\n",
    "print(df2)\n",
    "df2[['Fatal','Non-Fatal']].plot(kind='bar',\n",
    "                  stacked=True,\n",
    "                  figsize=(8,6),\n",
    "                  colormap='winter',\n",
    "                  title='Urban/Rural and Accident Severity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacked percentage\n",
    "#round(df_balanced.groupby(['Urban_or_Rural_Area'])['Accident_Severity'].value_counts(normalize=True)*100,2)\n",
    "df2_pct = df_balanced.groupby(['Urban_or_Rural_Area'])['Accident_Severity'].value_counts(normalize=True).unstack('Accident_Severity')\n",
    "df2_pct[['Fatal','Non-Fatal']].plot(kind='bar',\n",
    "                  stacked=True,\n",
    "                  figsize=(8,6),\n",
    "                  colormap='winter',\n",
    "                  title='Urban/Rural and Accident Severity, Stacked %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Light Conditions and Accident Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacked histogram\n",
    "df3 = df_balanced.groupby(['Light_Conditions', 'Accident_Severity'])['Light_Conditions'].count().unstack('Accident_Severity')\n",
    "df3[['Fatal','Non-Fatal']].plot(kind='bar',\n",
    "                  stacked=True,\n",
    "                  figsize=(8,6),\n",
    "                  colormap='winter',\n",
    "                  title='Light Conditions and Accident Severity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacked percentage\n",
    "#round(df_balanced.groupby(['Light_Conditions'])['Accident_Severity'].value_counts(normalize=True)*100,2)\n",
    "df3_pct = df_balanced.groupby(['Light_Conditions'])['Accident_Severity'].value_counts(normalize=True).unstack('Accident_Severity')\n",
    "df3_pct[['Fatal','Non-Fatal']].plot(kind='bar',\n",
    "                  stacked=True,\n",
    "                  figsize=(8,6),\n",
    "                  colormap='winter',\n",
    "                  title='Light Conditions and Accident Severity, Stacked %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Speed Limit and Accident Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacked histogram\n",
    "df4 = df_balanced.groupby(['Speed_limit', 'Accident_Severity'])['Speed_limit'].count().unstack('Accident_Severity')\n",
    "df4[['Fatal','Non-Fatal']].plot(kind='bar',\n",
    "                  stacked=True,\n",
    "                  figsize=(8,6),\n",
    "                  colormap='winter',\n",
    "                  title='Speed Limit and Accident Severity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacked percentage\n",
    "#round(df_balanced.groupby(['Light_Conditions'])['Accident_Severity'].value_counts(normalize=True)*100,2)\n",
    "df4_pct = df_balanced.groupby(['Speed_limit'])['Accident_Severity'].value_counts(normalize=True).unstack('Accident_Severity')\n",
    "df4_pct[['Fatal','Non-Fatal']].plot(kind='bar',\n",
    "                  stacked=True,\n",
    "                  figsize=(8,6),\n",
    "                  colormap='winter',\n",
    "                  title='Speed Limit and Accident Severity, Stacked %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection and Train/Test Split\n",
    "\n",
    "Four of the features most strongly correlated (negatively and positively) with the target variable (Accident_Severity) are retained. Variables are then converted using one hot encoding, so that each label is binary.\n",
    "\n",
    "X and y datasets are created, from the feature set and target variable, respectively.\n",
    "\n",
    "Using Scikit Learn's train_test_split, the datasets are split with 20% being used as a test set.\n",
    "\n",
    "Note, since the model we will be using is a logistic regression model, data normalization is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_balanced[['Weather_Conditions','Light_Conditions','Urban_or_Rural_Area','Speed_limit']]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X,pd.get_dummies(df_balanced['Weather_Conditions'])], axis=1)\n",
    "X.drop(['Weather_Conditions'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X,pd.get_dummies(df_balanced['Light_Conditions'])], axis=1)\n",
    "X.drop(['Light_Conditions'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X,pd.get_dummies(df_balanced['Urban_or_Rural_Area'])], axis=1)\n",
    "X.drop(['Urban_or_Rural_Area'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_balanced['Accident_Severity'].values\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "#X_train= preprocessing.StandardScaler().fit(X_train).transform(X_train)\n",
    "#X_test= preprocessing.StandardScaler().fit(X_test).transform(X_test)\n",
    "\n",
    "print(X_test[0:5])\n",
    "\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Model: Logistic Regression\n",
    "\n",
    "Logistic Regression is chosen because it is an efficient way to model very large datasets, where probability of outcome is a useful datapoint. In this situation, where accident severity is predicted, this probability could help the user make a more informed decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR_Model = LogisticRegression(C=0.1, solver='saga').fit(X_train,y_train)\n",
    "LR_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_yhat = LR_Model.predict(X_test)\n",
    "LR_yhat_prob = LR_Model.predict_proba(X_test)\n",
    "print(\"LR Jaccard index: %.2f\" % jaccard_similarity_score(y_test, LR_yhat))\n",
    "print(\"LR F1-score: %.2f\" % f1_score(y_test, LR_yhat, average='weighted') )\n",
    "print(\"LR LogLoss: %.2f\" % log_loss(y_test, LR_yhat_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Our model seems to be moderately inaccurate. A confusion matrix can help us understand which cases are being inaccurately classified in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "print(confusion_matrix(y_test, LR_yhat, labels=['Fatal','Non-Fatal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, LR_yhat, labels=['Fatal','Non-Fatal'])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Accident=Fatal','Accident=Non-Fatal'],normalize=True,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, LR_yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool for user: predicting accident severity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select your conditions:\n",
    "\n",
    "**Weather Conditions:**\n",
    "* Fine no high winds\n",
    "* Raining no high winds\n",
    "* Snowing no high winds\n",
    "* Fine + high winds\n",
    "* Raining + high winds\n",
    "* Snowing + high winds\n",
    "* Fog or mist\n",
    "\n",
    "\n",
    "**Light Conditions**\n",
    "* Daylight\n",
    "* Darkness - lights lit\n",
    "* Darkness - lights unlit\n",
    "* Darkness - no lighting\n",
    "\n",
    "**Urban or Rural**\n",
    "* Urban\n",
    "* Rural\n",
    "\n",
    "**Speed Limit**\n",
    "* 20.0\n",
    "* 30.0\n",
    "* 40.0\n",
    "* 50.0\n",
    "* 60.0\n",
    "* 70.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_Severity(Model, Weather_Conditions,Light_Conditions,Urban_or_Rural_Area,Speed_Limit):\n",
    "    \"\"\"\n",
    "    This function predicts and prints whether your accident would be fatal or non-fatal,\n",
    "    given a set of inputs (where all except Speed Limit are string; Speed Limit = float\n",
    "    \n",
    "    Model should be the Logistic Regression model we created\n",
    "    \n",
    "    \"\"\"\n",
    "    column_names = ['Speed_Limit',\n",
    "                    'Fine + high winds',\n",
    "                    'Fine no high winds',\n",
    "                    'Fog or mist',\n",
    "                    'Raining + high winds',\n",
    "                    'Raining no high winds',\n",
    "                    'Snowing + high winds',\n",
    "                    'Snowing no high winds',\n",
    "                    'Darkness - lights lit',\n",
    "                    'Darkness - lights unlit',\n",
    "                    'Darkness - no lighting',\n",
    "                    'Daylight',\n",
    "                    'Rural',\n",
    "                    'Urban']\n",
    "    \n",
    "    input_df = pd.DataFrame(columns = column_names)\n",
    "    input_df.loc[0] = [0]*len(column_names)\n",
    "    \n",
    "    input_df.loc[0, Weather_Conditions] = 1\n",
    "    input_df.loc[0, Light_Conditions] = 1\n",
    "    input_df.loc[0, Urban_or_Rural_Area] = 1\n",
    "    input_df.loc[0, 'Speed_Limit'] = Speed_Limit\n",
    "    \n",
    "    X_input = [np.array(input_df.iloc[0])]\n",
    "    \n",
    "    Y_Predicted = Model.predict(X_input)\n",
    "    Y_Predicted_Prob = Model.predict_proba(X_input)\n",
    "    \n",
    "    return (Y_Predicted, Y_Predicted_Prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Predict_Severity(LR_Model,'Snowing + high winds','Darkness - no lighting', 'Rural', 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Predict_Severity(LR_Model,'Fine no high winds','Daylight', 'Urban', 30.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
